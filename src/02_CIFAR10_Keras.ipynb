{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 22\n",
    "plt.rc('font', size=MEDIUM_SIZE)            # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)        # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)        # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)       # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)       # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)       # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)     # fontsize of the figure title\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Subroutines\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Additional configurations, @see config.py\n",
    "import config\n",
    "\n",
    "### Configurations\n",
    "# Training-Size\n",
    "num_train = config.num_train                   # 60000 for full data set \n",
    "num_test  = config.num_test                    # 10000 for full data set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple functions to log information\n",
    "path = os.getcwd()+\"/log\"\n",
    "logDir = os.path.exists(path)\n",
    "if not logDir:\n",
    "    os.makedirs(path)\n",
    "\n",
    "plots = os.getcwd()+\"/plots\"\n",
    "logDir = os.path.exists(plots)\n",
    "if not logDir:\n",
    "    os.makedirs(plots)\n",
    "\n",
    "training_results = path+\"/keras-nn-training-log.txt\"\n",
    "def log_training_results(*s):\n",
    "    with open(training_results, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "hyperparameter_search_log = path+\"/keras-nn-hyperparameter-tuning-log.txt\"\n",
    "def log_hyperparameter_search(*s):\n",
    "    with open(hyperparameter_search_log, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "print(\"Generated data will be located in \", training_results, hyperparameter_search_log)\n",
    "print(\"Generated plots will be located in \", plots)\n",
    "\n",
    "log_training_results(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "if config.hyper_parameter_search:\n",
    "    log_hyperparameter_search(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch CIFAR10-Data from Keras repository\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X,  Y, RGB)\")\n",
    "print(\"Shape of training data:\\t\\t\", X_train.shape)\n",
    "print(\"Shape of training labels:\\t\", y_train.shape)\n",
    "print(\"Shape of testing data:\\t\\t\", X_test.shape)\n",
    "print(\"Shape of testing labels:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "cols=8\n",
    "rows=4\n",
    "fig, ax = plt.subplots(ncols=cols, nrows=rows, figsize=(cols*2, rows*2))\n",
    "index = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].set_title(y_train[y_train[index][0]], fontsize=16)\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].axis('off')\n",
    "        index += 1\n",
    "plt.show()\n",
    "fig.savefig('plots/cifar10_examples.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_label = y_train\n",
    "test_data = X_test\n",
    "test_label = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data such that we have access to every pixel of the image\n",
    "train_data = X_train.astype('float32')\n",
    "train_label = y_train.astype(\"float32\")\n",
    "test_data = X_test.astype('float32')\n",
    "test_label = y_test.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know the RGB color code where different values produce various colors. It is also difficult to remember every color combination. \n",
    "# We already know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "# To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to a range of values from 0 to 1.\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the amount of columns to fit the necessary sizes required by the neural network\n",
    "train_label = keras.utils.to_categorical(train_label, config.num_classes)\n",
    "test_label = keras.utils.to_categorical(test_label, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an optional step, we decrease the training and testing data size, such that the algorithms perform their execution in acceptable time\n",
    "train_data = train_data[1:num_train,]\n",
    "train_label = train_label[1:num_train]\n",
    "\n",
    "test_data = test_data[1:num_test,]\n",
    "test_label = test_label[1:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X, Y, RGB )\")\n",
    "print(\"Reshaped training data:\\t\\t\", train_data.shape)\n",
    "print(\"Reshaped training labels:\\t\", train_label.shape)\n",
    "print(\"Reshaped testing data:\\t\\t\", test_data.shape)\n",
    "print(\"Reshaped testing labels:\\t\", test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model: https://keras.io/guides/sequential_model/\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation='relu', padding='same')) \n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# Flatten input into feature vector and feed into dense layer\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Outputs from dense layer are projected onto 10 unit output layer\n",
    "model.add(Dense(units=config.num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "optimizer = keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0001,\n",
    "    epsilon = 1e-6,\n",
    ")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    x=train_data, \n",
    "    y=train_label, \n",
    "    batch_size=config.batch_size, \n",
    "    epochs=config.num_epochs, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_data, test_label))\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "params = {\"Keras\":{}}\n",
    "log_training_results(\"Trained new model: %s in %s seconds\" % (params, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting train data -- execution time: %ss\" % (end_time))\n",
    "log_training_results(\"\\t[%s] -- Accuracy: %s; Loss: %s\" % (params, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_data, test_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting test data --  execution time: %ss\" % (end_time))\n",
    "log_training_results(\"\\t[%s] -- Accuracy: %s; Loss: %s\" % (params, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let model predict data\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimation over correct and incorrect prediction via confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('CIFAR-10-Keras Confusion Matrix of standard NN')\n",
    "fig.savefig('plots/ConfusionMatrix_standard.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.hyper_parameter_search:\n",
    "    print(\"Terminating without hyperparameter search.\")\n",
    "    exit(0)\n",
    "print(\"Starting hyperparameter search over %s epochs each\" % (config.hps_max_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Space Review\n",
    "\n",
    "What can be tuned:\n",
    "- Dropout: If dropout happens & dropout rate\n",
    "- Convolutionals: Filters\n",
    "- Dense: Units\n",
    "- All: Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=64)\n",
    "    hp_activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    hp_dropout = hp.Boolean(\"dropout\")\n",
    "    hp_dropout_rate = hp.Float('dropout-rate', min_value=0.25, max_value=0.50, step=0.25)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation='relu', padding='same')) \n",
    "\n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "    # Max pooling layer\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "\n",
    "    # Flatten input into feature vector and feed into dense layer\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(units=hp_units, activation=hp_activation))\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "        \n",
    "    #model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "    model.add(Dense(units=hp_units, activation=hp_activation))\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "    # Outputs from dense layer are projected onto 10 unit output layer\n",
    "    model.add(Dense(units=config.num_classes, activation=\"softmax\"))\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.RMSprop(\n",
    "        learning_rate=hp_learning_rate,\n",
    "        epsilon = 1e-6,\n",
    "    )\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #model.compile(\n",
    "    #    optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "    #    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #    metrics=['accuracy']\n",
    "    #)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=config.hps_max_epochs,\n",
    "    factor=3,                    \n",
    "    directory='log',\n",
    "    project_name='keras-hyperparameter-search'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    epochs=config.hps_max_epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "log_hyperparameter_search(\"--- [%s] Running Parameter-Tests [SKLEARN-NN] ---\" % datetime.now())\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "log_hyperparameter_search(\"\\tBest parameters set found on following development set:\", best_hps.values)\n",
    "\n",
    "#log_hyperparameter_search(\"\\t\\tAccuracy: %s\" % best_hps.get('val_accuracy'))\n",
    "#log_hyperparameter_search(\"\\t\\tLayer-Units: %s\" % best_hps.get('units'))\n",
    "#log_hyperparameter_search(\"\\t\\tLearning Rate: %s\" % best_hps.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_data, train_label, epochs=config.num_epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_data, test_label)\n",
    "print(\"[test loss, test accuracy]:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let model predict data\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimation over correct and incorrect prediction via confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('CIFAR-10-Keras Confusion Matrix of optimal NN')\n",
    "fig.savefig('plots/ConfusionMatrix_optimal.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e2b2785e650337f79381cd4c5df08c4d5dc4623a6a0d2da7e01465b331d0fcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

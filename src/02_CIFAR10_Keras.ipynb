{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10-KERAS-NN\n",
    "\n",
    "The following script executes a program for picture recognition on the CIFAR-10 dataset using keras neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 22\n",
    "plt.rc('font', size=MEDIUM_SIZE)            # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)        # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)        # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)       # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)       # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)       # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)     # fontsize of the figure title\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Subroutines\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Additional configurations, @see config.py\n",
    "import config\n",
    "\n",
    "### Configurations\n",
    "# Training-Size\n",
    "num_train = config.num_train                   # 60000 for full data set \n",
    "num_test  = config.num_test                    # 10000 for full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple functions to log information\n",
    "path = os.getcwd()+\"/log\"\n",
    "logDir = os.path.exists(path)\n",
    "if not logDir:\n",
    "    os.makedirs(path)\n",
    "\n",
    "plots = os.getcwd()+\"/log/plots\"\n",
    "logDir = os.path.exists(plots)\n",
    "if not logDir:\n",
    "    os.makedirs(plots)\n",
    "\n",
    "training_results = path+\"/keras-nn-training-log.txt\"\n",
    "def log_training_results(*s):\n",
    "    with open(training_results, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "hyperparameter_search_log = path+\"/keras-nn-hyperparameter-tuning-log.txt\"\n",
    "def log_hyperparameter_search(*s):\n",
    "    with open(hyperparameter_search_log, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "print(\"Generated data will be located in \", training_results, hyperparameter_search_log)\n",
    "print(\"Generated plots will be located in \", plots)\n",
    "\n",
    "log_training_results(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "if config.hyper_parameter_search:\n",
    "    log_hyperparameter_search(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch CIFAR10-Data from Keras repository\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X,  Y, RGB)\")\n",
    "print(\"Shape of training data:\\t\\t\", X_train.shape)\n",
    "print(\"Shape of training labels:\\t\", y_train.shape)\n",
    "print(\"Shape of testing data:\\t\\t\", X_test.shape)\n",
    "print(\"Shape of testing labels:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "cols=8\n",
    "rows=4\n",
    "fig, ax = plt.subplots(ncols=cols, nrows=rows, figsize=(cols*2, rows*2))\n",
    "index = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].set_title(y_train[y_train[index][0]], fontsize=16)\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].axis('off')\n",
    "        index += 1\n",
    "plt.show()\n",
    "fig.savefig(plots+'/cifar10_examples.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_label = y_train\n",
    "test_data = X_test\n",
    "test_label = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data such that we have access to every pixel of the image\n",
    "train_data = X_train.astype('float32')\n",
    "train_label = y_train.astype(\"float32\")\n",
    "test_data = X_test.astype('float32')\n",
    "test_label = y_test.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know the RGB color code where different values produce various colors. It is also difficult to remember every color combination. \n",
    "# We already know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "# To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to a range of values from 0 to 1.\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the labels by conversion from integers to a class matrix\n",
    "train_label = keras.utils.to_categorical(train_label, config.num_classes)\n",
    "test_label = keras.utils.to_categorical(test_label, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an optional step, we decrease the training and testing data size, such that the algorithms perform their execution in acceptable time\n",
    "train_data = train_data[1:num_train,]\n",
    "train_label = train_label[1:num_train]\n",
    "\n",
    "test_data = test_data[1:num_test,]\n",
    "test_label = test_label[1:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X, Y, RGB )\")\n",
    "print(\"Reshaped training data:\\t\\t\", train_data.shape)\n",
    "print(\"Reshaped training labels:\\t\", train_label.shape)\n",
    "print(\"Reshaped testing data:\\t\\t\", test_data.shape)\n",
    "print(\"Reshaped testing labels:\\t\", test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Neural Network Model\n",
    "\n",
    "Groups layers into a Tensorflow-defined neural network and provides interfaces for training and inference.\n",
    "\n",
    "For more information, see: https://keras.io/api/models/sequential/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create model: https://keras.io/guides/sequential_model/\n",
    "model = Sequential()\n",
    "\n",
    "### Minimum Model\n",
    "#model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation='relu', padding='same')) \n",
    "#model.add(Flatten()) \n",
    "#model.add(Dense(units=config.num_classes, activation=\"softmax\"))\n",
    "\n",
    "### Standard model\n",
    "# First convolutional layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation='relu', padding='same')) \n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "# Flatten input into feature vector and feed into dense layer\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Outputs from dense layer are projected onto 10 unit output layer\n",
    "model.add(Dense(units=config.num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "optimizer = keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0001,\n",
    "    epsilon = 1e-6,\n",
    ")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    x=train_data, \n",
    "    y=train_label, \n",
    "    batch_size=config.batch_size, \n",
    "    epochs=config.num_epochs, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_data, test_label))\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "params = {\"Keras\":{\"batch_size\":config.batch_size, \"epochs\":config.num_epochs}}\n",
    "log_training_results(\"--- [%s] Trained new model: %s in %s seconds ---\" % (datetime.now(), params, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig(plots+'/training_history_standard.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting train data -- Execution time: %ss; Accuracy: %s; Loss: %s\" % (end_time, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_data, test_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting test data -- Execution time: %ss; Accuracy: %s; Loss: %s\" % (end_time, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let model predict data\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimation over correct and incorrect prediction via confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('CIFAR-10 Keras Confusion Matrix of standard NN')\n",
    "plt.show()\n",
    "fig.savefig(plots+'/ConfusionMatrix_standard.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.hyper_parameter_search:\n",
    "    print(\"Terminating without hyperparameter search.\")\n",
    "    exit(0)\n",
    "print(\"Starting hyperparameter search over %s epochs each\" % (config.hps_max_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter evaluation\n",
    "\n",
    "The search utilizes the keras_tuner library. The model_builder defines the layout and parametrization which are utilized by the different search algorithms further below. \n",
    "\n",
    "What can be tuned:\n",
    "- Dropout: If dropout happens & dropout rate\n",
    "- Convolutionals: Filters\n",
    "- Dense: Units\n",
    "- All: Activation functions\n",
    "\n",
    "Also see: https://keras.io/api/keras_tuner/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Hopelessly overengineered, but fun\n",
    "# Takes too damn long\n",
    "# Debatable whether this might produce any meaningful data\n",
    "# Gives a fair overview over what can be configured for HPS\n",
    "def model_builder(hp):\n",
    "    ### Base Model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    ### Layers    \n",
    "    # MANDATORY :: First convolutional layer\n",
    "    hp_first_conv2d_activation = hp.Choice('First Conv2D activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation=hp_first_conv2d_activation, padding='same')) \n",
    "\n",
    "    # OPTIONAL :: Second convolutional layer\n",
    "    hp_second_conv2d = hp.Boolean(\"Use Second Conv2D\")\n",
    "    if hp_second_conv2d:\n",
    "        hp_second_conv2d_activation = hp.Choice('Second Conv2D activation', ['relu', 'tanh', 'sigmoid'])\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=hp_second_conv2d_activation))\n",
    "\n",
    "    # OPTIONAL :: Max pooling layer\n",
    "    hp_maxpooling2d = hp.Boolean(\"Use MaxPooling2D\")\n",
    "    if hp_maxpooling2d:\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "        \n",
    "    # OPTIONAL :: Dropout\n",
    "    hp_first_dropout = hp.Boolean(\"Use First Dropout\")\n",
    "    if hp_first_dropout:\n",
    "        hp_first_dropout_rate = hp.Float('First dropout rate', min_value=0.25, max_value=0.50, step=0.05)\n",
    "        model.add(Dropout(rate=hp_first_dropout_rate))\n",
    "\n",
    "    # MANDATORY :: Flatten input into feature vector and feed into dense layer\n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    # OPTIONAL :: First dense layer \n",
    "    hp_first_dense = hp.Boolean(\"Use First Dense\")\n",
    "    if hp_first_dense:\n",
    "        hp_first_dense_activation = hp.Choice('First dense activation', ['relu', 'tanh', 'sigmoid'])\n",
    "        hp_first_dense_units = hp.Int('First dense units', min_value=32, max_value=512, step=32)\n",
    "        model.add(Dense(units=hp_first_dense_units, activation=hp_first_dense_activation))\n",
    "    \n",
    "    # OPTIONAL :: Dropout\n",
    "    hp_second_dropout = hp.Boolean(\"Use Second Dropout\")\n",
    "    if hp_second_dropout:\n",
    "        hp_second_dropout_rate = hp.Float('Second dropout rate', min_value=0.25, max_value=0.50, step=0.05)\n",
    "        model.add(Dropout(rate=hp_second_dropout_rate))\n",
    "        \n",
    "    # OPTIONAL :: Second dense layer\n",
    "    hp_second_dense = hp.Boolean(\"Use Second Dense\")\n",
    "    if hp_second_dense:\n",
    "        hp_second_dense_activation = hp.Choice('Second dense activation', ['relu', 'tanh', 'sigmoid'])\n",
    "        hp_second_dense_units = hp.Int('Second dense units', min_value=32, max_value=512, step=32)\n",
    "        model.add(Dense(units=hp_second_dense_units, activation=hp_second_dense_activation))\n",
    "\n",
    "    # OPTIONAL :: Dropout\n",
    "    hp_third_dropout = hp.Boolean(\"Use Third Dropout\")\n",
    "    if hp_third_dropout:\n",
    "        hp_third_dropout_rate = hp.Float('Third dropout rate', min_value=0.25, max_value=0.50, step=0.05)\n",
    "        model.add(Dropout(rate=hp_third_dropout_rate))\n",
    "\n",
    "    # MANDATORY :: Outputs from dense layer are projected onto 10 unit output layer\n",
    "    hp_third_dense_activation = hp.Choice('Third activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    model.add(Dense(units=config.num_classes, activation=hp_third_dense_activation))\n",
    "\n",
    "    # Compile model\n",
    "    hp_learning_rate = hp.Choice('Optimizer learnrate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    optimizer = keras.optimizers.RMSprop(\n",
    "        learning_rate=hp_learning_rate,\n",
    "        epsilon = 1e-6,\n",
    "    )\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Base Model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Hyperparameters\n",
    "    hp_secondary_conv2d = True#hp.Boolean(\"Use 2nd Conv. Layer\")\n",
    "    hp_conv_activations = hp.Choice('Conv2D Activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    hp_use_maxpooling2d = True#hp.Boolean(\"Use MaxPooling2D\")\n",
    "\n",
    "    hp_dropout = True#hp.Boolean(\"Use dropout layers\")\n",
    "    hp_dropout_rate = hp.Float('Dropout rate', min_value=0.25, max_value=0.50, step=0.05)\n",
    "\n",
    "    hp_use_dense = True#hp.Boolean(\"Use dense layers\")\n",
    "    hp_dense_units = hp.Int('Dense Units', min_value=32, max_value=512, step=32)\n",
    "    hp_dense_activations = hp.Choice('Dense Activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('Learning rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32,32,3), activation=hp_conv_activations, padding='same')) # Usually RELU\n",
    "\n",
    "    # Second convolutional layer\n",
    "    if hp_secondary_conv2d:\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=hp_conv_activations)) # Usually RELU\n",
    "\n",
    "    # Max pooling layer\n",
    "    if hp_use_maxpooling2d:\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "\n",
    "    # Flatten input into feature vector and feed into dense layer\n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    if hp_use_dense:\n",
    "        model.add(Dense(units=hp_dense_units, activation=hp_dense_activations))\n",
    "\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "        \n",
    "    #model.add(Dense(units=config.num_of_units, activation='relu'))\n",
    "    if hp_use_dense:\n",
    "        model.add(Dense(units=hp_dense_units, activation=hp_dense_activations))\n",
    "\n",
    "    if hp_dropout:\n",
    "        model.add(Dropout(rate=hp_dropout_rate))\n",
    "    \n",
    "    # Outputs from dense layer are projected onto 10 unit output layer\n",
    "    model.add(Dense(units=config.num_classes, activation=hp_dense_activations)) # Usually softmax\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.RMSprop(\n",
    "        learning_rate=hp_learning_rate,\n",
    "        epsilon = 1e-6,\n",
    "    )\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "See: https://keras.io/api/keras_tuner/tuners/random/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    #max_epochs=config.max_trials,\n",
    "    #factor=3,                    \n",
    "    directory='log/hps',\n",
    "    project_name='keras-hyperparameter-search-RandomSearch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)\n",
    "csvlogger = keras.callbacks.CSVLogger(hyperparameter_search_log, separator=\",\", append=True)\n",
    "tuner.search(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    epochs=config.hps_max_epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "log_hyperparameter_search(\"--- [%s] Running Parameter-Tests [Keras-RandomSearch] ---\" % datetime.now())\n",
    "best_hps_rs = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "log_hyperparameter_search(\"Best parameters set found on following development set: %s\" % best_hps_rs.values)\n",
    "best_hps_rs_results = tuner.results_summary(num_trials=1)\n",
    "best_hps_rs_model = tuner.get_best_models(num_models=1)[0]\n",
    "rs_test_loss, rs_test_acc = best_hps_rs_model.evaluate(test_data, test_label)\n",
    "log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (rs_test_acc, rs_test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_hps = tuner.get_best_hyperparameters(3)\n",
    "top3_models = tuner.get_best_models(3)\n",
    "for i in range(1, 3):\n",
    "    log_hyperparameter_search(\"Additional parameters set found on following development set: %s\" % top3_hps[i].values)\n",
    "    loss, acc = top3_models[i].evaluate(test_data, test_label)\n",
    "    log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (acc, loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimiziation\n",
    "\n",
    "See: https://keras.io/api/keras_tuner/tuners/bayesian/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=config.hps_max_trials,\n",
    "    #factor=3,                    \n",
    "    directory='log/hps',\n",
    "    project_name='keras-hyperparameter-search-BayesianOptimization'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)\n",
    "csvlogger = keras.callbacks.CSVLogger(hyperparameter_search_log, separator=\",\", append=True)\n",
    "tuner.search(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    epochs=config.hps_max_epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "log_hyperparameter_search(\"--- [%s] Running Parameter-Tests [Keras-BayesianOptimization] ---\" % datetime.now())\n",
    "best_hps_bo = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "log_hyperparameter_search(\"Best parameters set found on following development set: %s\" % best_hps_bo.values)\n",
    "\n",
    "best_hps_bo_results = tuner.results_summary(num_trials=1)\n",
    "\n",
    "best_hps_bo_model = tuner.get_best_models(num_models=1)[0]\n",
    "bo_test_loss, bo_test_acc = best_hps_bo_model.evaluate(test_data, test_label)\n",
    "log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (bo_test_acc, bo_test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_hps = tuner.get_best_hyperparameters(3)\n",
    "top3_models = tuner.get_best_models(3)\n",
    "for i in range(1, 3):\n",
    "    log_hyperparameter_search(\"Additional parameters set found on following development set: %s\" % top3_hps[i].values)\n",
    "    loss, acc = top3_models[i].evaluate(test_data, test_label)\n",
    "    log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (acc, loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperband\n",
    "\n",
    "See: https://keras.io/api/keras_tuner/tuners/hyperband/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=config.hps_max_trials,\n",
    "    factor=3,                    \n",
    "    directory='log/hps',\n",
    "    project_name='keras-hyperparameter-search-Hyperband'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)\n",
    "csvlogger = keras.callbacks.CSVLogger(hyperparameter_search_log, separator=\",\", append=True)\n",
    "tuner.search(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    epochs=config.hps_max_epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "log_hyperparameter_search(\"--- [%s] Running Parameter-Tests [Keras-Hyperband] ---\" % datetime.now())\n",
    "best_hps_hb = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "log_hyperparameter_search(\"Best parameters set found on following development set: %s\" % best_hps_hb.values)\n",
    "\n",
    "best_hps_hb_results = tuner.results_summary(num_trials=1)\n",
    "\n",
    "best_hps_hb_model = tuner.get_best_models(num_models=1)[0]\n",
    "hb_test_loss, hb_test_acc = best_hps_hb_model.evaluate(test_data, test_label)\n",
    "log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (hb_test_acc, hb_test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_hps = tuner.get_best_hyperparameters(3)\n",
    "top3_models = tuner.get_best_models(3)\n",
    "for i in range(1, 3):\n",
    "    log_hyperparameter_search(\"Additional parameters set found on following development set: %s\" % top3_hps[i].values)\n",
    "    loss, acc = top3_models[i].evaluate(test_data, test_label)\n",
    "    log_hyperparameter_search(\"\\tPredicting test data -- Accuracy: %s; Loss: %s\" % (acc, loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search analytics\n",
    "\n",
    "Now utilize the 'best' algorithm (weight based on accuracy) and train a new model with the most optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map and print the scores\n",
    "accuracy_score = {\"RandomSearch\": rs_test_acc, \"BayesianOptimization\": bo_test_acc, \"HyperBand\": hb_test_acc}\n",
    "loss_score = {\"RandomSearch\": rs_test_loss, \"BayesianOptimization\": bo_test_loss, \"HyperBand\": hb_test_loss}\n",
    "\n",
    "print(\"--- Finalized scores \")\n",
    "print(\"\\tAccuracy: %s\" % accuracy_score)\n",
    "print(\"\\tLoss: %s\" % loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model out of all three based on accuracy\n",
    "# For Loss, it is required to adjust the score here and the values taken for the diagrams below.\n",
    "# Tip: Search ''accuracy'' and replace\n",
    "best_accuracy = max(accuracy_score, key=accuracy_score.get)\n",
    "print(\"Algorithm: %s\" % best_accuracy)\n",
    "\n",
    "best_accuracy_value = max(accuracy_score.values())\n",
    "print(\"Score: %s\" % best_accuracy_value)\n",
    "\n",
    "model = 0\n",
    "if best_accuracy == \"RandomSearch\":\n",
    "    model = tuner.hypermodel.build(best_hps_rs)\n",
    "elif best_accuracy == \"BayesianOptimization\":\n",
    "    model = tuner.hypermodel.build(best_hps_bo)\n",
    "elif best_accuracy == \"HyperBand\":\n",
    "    model = tuner.hypermodel.build(best_hps_hb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model Fitting\n",
    "\n",
    "Train the most-optimal model the algorithm could come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model with the optimal algorithm and parameters\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    x=train_data, \n",
    "    y=train_label, \n",
    "    batch_size=config.batch_size, \n",
    "    epochs=config.num_epochs, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_data, test_label)\n",
    ")\n",
    "end_time = time.time() - start_time\n",
    "params = {\"HPS-Opt-Keras\":{\"batch_size\":config.batch_size, \"epochs\":config.num_epochs}}\n",
    "log_training_results(\"--- [%s] Trained new model: %s in %s seconds ---\" % (datetime.now(), params, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig(plots+'/training_history_optimal.png')\n",
    "\n",
    "# When a machine learning model has high training accuracy and very low validation then this case is probably known as over-fitting. The reasons for this can be as follows:\n",
    "#    The hypothesis function you are using is too complex that your model perfectly fits the training data but fails to do on test/validation data.\n",
    "#    The number of learning parameters in your model is way too big that instead of generalizing the examples , your model learns those examples and hence the model performs badly on test/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "log_training_results(\"\\tPredicting train data -- Execution time: %ss; Accuracy: %s; Loss: %s\" % (end_time, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_data, test_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting test data -- Execution time: %ss; Accuracy: %s; Loss: %s\" % (end_time, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let model predict data\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimation over correct and incorrect prediction via confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('CIFAR-10 Keras Confusion Matrix of optimal NN')\n",
    "plt.show()\n",
    "fig.savefig(plots+'/ConfusionMatrix_optimal.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e2b2785e650337f79381cd4c5df08c4d5dc4623a6a0d2da7e01465b331d0fcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

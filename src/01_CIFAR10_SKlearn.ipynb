{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10-SKLEARN-NN\n",
    "\n",
    "The following script executes a program for picture recognition on the CIFAR-10 dataset using SKLearn multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Subroutines\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Additional configurations, @see config.py\n",
    "import config\n",
    "\n",
    "# Training-Sizes\n",
    "num_train = config.num_train\n",
    "num_test  = config.num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple functions to log information\n",
    "path = os.getcwd()+\"/log\"\n",
    "logDir = os.path.exists(path)\n",
    "if not logDir:\n",
    "    os.makedirs(path)\n",
    "\n",
    "training_results = path+\"/sklearn-nn-training-log.txt\"\n",
    "def log_training_results(*s):\n",
    "    with open(training_results, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "hyperparameter_search_log = path+\"/sklearn-nn-hyperparameter-tuning-log.txt\"\n",
    "def log_hyperparameter_search(*s):\n",
    "    with open(hyperparameter_search_log, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "print(\"Generated data will be located in \", training_results, hyperparameter_search_log)\n",
    "log_training_results(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "if config.hyper_parameter_search:\n",
    "    log_hyperparameter_search(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch CIFAR10-Data from Keras repository\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X,  Y, RGB)\")\n",
    "print(\"Shape of training data:\\t\\t\", X_train.shape)\n",
    "print(\"Shape of training labels:\\t\", y_train.shape)\n",
    "print(\"Shape of testing data:\\t\\t\", X_test.shape)\n",
    "print(\"Shape of testing labels:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "cols=8\n",
    "rows=4\n",
    "fig, ax = plt.subplots(ncols=cols, nrows=rows, figsize=(cols*2, rows*2))\n",
    "index = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].set_title(y_train[y_train[index][0]], fontsize=16)\n",
    "        ax[i,j].imshow(X_train[index])\n",
    "        ax[i,j].axis('off')\n",
    "        index += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_label = y_train\n",
    "test_data = X_test\n",
    "test_label = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data such that we have access to every pixel of the image\n",
    "# The reason to access every pixel is that only then we can apply deep learning ideas and can assign color code to every pixel.\n",
    "# https://datascience.stackexchange.com/questions/24459/how-to-give-cifar-10-as-an-input-to-mlp\n",
    "train_data = X_train.reshape(X_train.shape[0], 32*32*3).astype('float32')\n",
    "train_label = y_train.astype(\"float32\")\n",
    "test_data = X_test.reshape(X_test.shape[0], 32*32*3).astype('float32')\n",
    "test_label = y_test.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know the RGB color code where different values produce various colors. It is also difficult to remember every color combination. \n",
    "# We already know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "# To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to a range of values from 0 to 1.\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an optional step, we decrease the training and testing data size, such that the algorithms perform their execution in acceptable time\n",
    "train_data = train_data[1:num_train]\n",
    "train_label = train_label[1:num_train]\n",
    "\n",
    "test_data = test_data[1:num_test]\n",
    "test_label = test_label[1:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t (Sets,  X*Y*RGB)\")\n",
    "print(\"Reshaped training data:\\t\\t\", train_data.shape)\n",
    "print(\"Reshaped training labels:\\t\", train_label.shape)\n",
    "print(\"Reshaped testing data:\\t\\t\", test_data.shape)\n",
    "print(\"Reshaped testing labels:\\t\", test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron classifier\n",
    "\n",
    "This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "\n",
    "For more information, see: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),      # The ith element represents the number of neurons in the ith hidden layer.\n",
    "    activation='relu',              # Activation function for the hidden layer.\n",
    "    solver='adam',                  # The solver for weight optimization.\n",
    "    alpha=0.0001,                   # Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "    batch_size='auto',              # Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples).\n",
    "    learning_rate='constant',       # Learning rate schedule for weight updates.\n",
    "    learning_rate_init=0.001,       # The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
    "    power_t=0.5,                    # The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.\n",
    "    max_iter=200,                   # Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
    "    shuffle=True,                   # Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’.\n",
    "    random_state=None,              # Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
    "    tol=0.0001,                     # Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.\n",
    "    verbose=False,                  # Whether to print progress messages to stdout.\n",
    "    warm_start=False,               # When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.\n",
    "    momentum=0.9,                   # Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.\n",
    "    nesterovs_momentum=True,        # Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0.\n",
    "    early_stopping=False,           # Whether to use early stopping to terminate training when validation score is not improving. \n",
    "    validation_fraction=0.1,        # The proportion of training data to set aside as validation set for early stopping. Only used if early_stopping is True.\n",
    "    beta_1=0.9,                     # Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=’adam’.\n",
    "    beta_2=0.999,                   # Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=’adam’.\n",
    "    epsilon=1e-08,                  # Value for numerical stability in adam. Only used when solver=’adam’.\n",
    "    n_iter_no_change=10,            # Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’.\n",
    "    max_fun=15000                   # Only used when solver=’lbfgs’. Maximum number of loss function calls. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the multi-layer perceptron classifier and fit to datasets.\n",
    "mlp = MLPClassifier(\n",
    "    batch_size=config.batch_size,\n",
    "    max_iter=config.num_epochs,\n",
    "    hidden_layer_sizes=config.num_of_units\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "#   train_data ... ndarray or sparse matrix of shape (n_samples, n_features), the input data\n",
    "#   train_label ... ndarray of shape (n_samples,) or (n_samples, n_outputs), the target values (class labels in classification, real numbers in regression)\n",
    "mlp.fit(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "params = {\"MLP\":{'batch_size':mlp.get_params()[\"batch_size\"], 'num_epochs':mlp.get_params()[\"max_iter\"], 'num_of_units':mlp.get_params()[\"hidden_layer_sizes\"], 'activation':mlp.get_params()[\"activation\"], 'alpha':mlp.get_params()[\"alpha\"], 'epsilon':mlp.get_params()[\"epsilon\"]}}\n",
    "log_training_results(\"Trained new model: %s in %s seconds\" % (params, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the multi-layer perceptron classifier.\n",
    "start_time = time.time()\n",
    "#   train_data ... {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "predictions = mlp.predict(train_data)\n",
    "end_time = time.time() - start_time\n",
    "log_training_results(\"\\tPredicting train data -- execution time: %ss\" % (end_time))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = mlp.predict(test_data)\n",
    "end_time = time.time() - start_time\n",
    "log_training_results(\"\\tPredicting test data -- execution time: %ss\" % (end_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "start_time = time.time()\n",
    "score = mlp.score(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "log_training_results(\"\\t[Train-data x %s] -- mean accuracy: %s; execution time: %ss\" % (params, score, end_time))  \n",
    "\n",
    "start_time = time.time()\n",
    "score = mlp.score(test_data, test_label)\n",
    "end_time = time.time() - start_time\n",
    "log_training_results(\"\\t[Test-data x %s] -- mean accuracy: %s; execution time: %ss\" % (params, score, end_time))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter evaluation\n",
    "\n",
    "Search of optimal parameters using GridSearchCV and predefined values.\n",
    "\n",
    "For more information, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Hyperparameter search -- Takes up a long time.\n",
    "# 960 Fits for all params, takes 2 days lol\n",
    "# 240 Fits for reduced params, takes roughly 8 hours\n",
    "# From previous assignment, we know that relu and adam did best\n",
    "if config.hyper_parameter_search:\n",
    "    mlp = MLPClassifier()\n",
    "    parameters = {\n",
    "        'hidden_layer_sizes': [(64,),(128,),(256,),(512,)],\n",
    "        'max_iter': [10, 20],\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        #'activation': ['relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        #'solver': ['adam'],\n",
    "        'alpha': [0.001, 0.0001],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "    log_hyperparameter_search(\"--- [%s] Running Parameter-Tests [SKLEARN-NN] ---\" % datetime.now())\n",
    "    grid = GridSearchCV(estimator=mlp, param_grid=parameters, verbose=3, n_jobs=-1)\n",
    "    grid.fit(train_data, train_label)\n",
    "\n",
    "    log_hyperparameter_search(\"\\tBest parameters set found on following development set:\")\n",
    "    log_hyperparameter_search(\"\\t\\tSupport Vector: %s\" % grid.best_estimator_)\n",
    "    log_hyperparameter_search(\"\\t\\tSupport Vector Parametrization: %s\" % grid.best_params_)\n",
    "    log_hyperparameter_search(\"\\t\\tAsserted Score: %s\" % grid.best_score_)\n",
    "    log_hyperparameter_search(\"Total Score \\t\\t Configurations\")\n",
    "\n",
    "    means = grid.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid.cv_results_[\"std_test_score\"]\n",
    "    params = grid.cv_results_[\"params\"]\n",
    "    for mean, std, params in zip(means, stds, params):\n",
    "        log_hyperparameter_search(\"%0.3f (+/-%0.03f)\\t%r\" % (mean, std, params))\n",
    "    print(\"Wrote classifier comparisons to file \", hyperparameter_search_log)\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    \n",
    "    y_true, y_pred = test_label, grid.predict(test_data)\n",
    "    log_hyperparameter_search(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e2b2785e650337f79381cd4c5df08c4d5dc4623a6a0d2da7e01465b331d0fcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
